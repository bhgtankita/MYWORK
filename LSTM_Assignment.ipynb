{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhgtankita/MYWORK/blob/master/LSTM_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHkthFmSEJPp",
        "colab_type": "code",
        "outputId": "c22d492c-6999-49ae-c6dc-7defad89d975",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1cLS-TQELog",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ac7a7cac-7460-499f-d207-5e96a9548acc"
      },
      "source": [
        "# Load Larger LSTM network and generate text\n",
        "import sys\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "import re\n",
        "import string\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqmR-FxhEP0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"gdrive/My Drive/Gutenberg/wonderland.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reOuxaIEESGP",
        "colab_type": "code",
        "outputId": "bc1656e9-a221-406c-fb22-0074d49943d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer = Tokenizer(char_level=True)\n",
        "  \n",
        "tr = str.maketrans(\"\", \"\", string.punctuation)\n",
        "data = raw_text.translate(tr)\n",
        "  \n",
        "# basic cleanup\n",
        "corpus = [line for line in data.lower().split(\"\\n\") if line.strip() != '']\n",
        "# print(corpus)\n",
        "\n",
        "# tokenization\t\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "print(total_words)\n",
        "\n",
        "# create input sequences using list of tokens\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "#   print(token_list)\n",
        "  for i in range(1, len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)\n",
        "#     print(input_sequences)\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "label = np_utils.to_categorical(label, num_classes=total_words)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqnEQIKKEgVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.expand_dims(predictors, axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHr6jB0nIjGH",
        "colab_type": "code",
        "outputId": "aa262714-dfea-45bd-e23e-90308ba46a8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(X.shape)\n",
        "print(label.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(129978, 72, 1)\n",
            "(129978, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CzkU0ldEmpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(label.shape[1], activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpJo6tZzEnif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"gdrive/My Drive/Gutenberg/weights/weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDLlglEw3Iwe",
        "colab_type": "text"
      },
      "source": [
        "# Final loss after 100th epoch is 1.1009"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qk53mn_xEs9Y",
        "colab_type": "code",
        "outputId": "360cc93b-7a59-4b39-fab8-451e0190f0ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X, label, epochs=100, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0726 16:35:43.201711 139860711278464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0726 16:35:43.207754 139860711278464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0726 16:35:43.222575 139860711278464 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0726 16:35:43.245902 139860711278464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0726 16:35:44.046688 139860711278464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0726 16:35:44.225536 139860711278464 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "129978/129978 [==============================] - 286s 2ms/step - loss: 2.5305\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.53047, saving model to gdrive/My Drive/Gutenberg/weights/weights-01-2.5305.hdf5\n",
            "Epoch 2/100\n",
            "129978/129978 [==============================] - 281s 2ms/step - loss: 2.1196\n",
            "\n",
            "Epoch 00002: loss improved from 2.53047 to 2.11962, saving model to gdrive/My Drive/Gutenberg/weights/weights-02-2.1196.hdf5\n",
            "Epoch 3/100\n",
            "129978/129978 [==============================] - 279s 2ms/step - loss: 1.9479\n",
            "\n",
            "Epoch 00003: loss improved from 2.11962 to 1.94786, saving model to gdrive/My Drive/Gutenberg/weights/weights-03-1.9479.hdf5\n",
            "Epoch 4/100\n",
            "129978/129978 [==============================] - 281s 2ms/step - loss: 1.8442\n",
            "\n",
            "Epoch 00004: loss improved from 1.94786 to 1.84424, saving model to gdrive/My Drive/Gutenberg/weights/weights-04-1.8442.hdf5\n",
            "Epoch 5/100\n",
            "129978/129978 [==============================] - 281s 2ms/step - loss: 1.7735\n",
            "\n",
            "Epoch 00005: loss improved from 1.84424 to 1.77350, saving model to gdrive/My Drive/Gutenberg/weights/weights-05-1.7735.hdf5\n",
            "Epoch 6/100\n",
            "129978/129978 [==============================] - 281s 2ms/step - loss: 1.7202\n",
            "\n",
            "Epoch 00006: loss improved from 1.77350 to 1.72015, saving model to gdrive/My Drive/Gutenberg/weights/weights-06-1.7202.hdf5\n",
            "Epoch 7/100\n",
            "129978/129978 [==============================] - 280s 2ms/step - loss: 1.6760\n",
            "\n",
            "Epoch 00007: loss improved from 1.72015 to 1.67598, saving model to gdrive/My Drive/Gutenberg/weights/weights-07-1.6760.hdf5\n",
            "Epoch 8/100\n",
            "129978/129978 [==============================] - 281s 2ms/step - loss: 1.6394\n",
            "\n",
            "Epoch 00008: loss improved from 1.67598 to 1.63940, saving model to gdrive/My Drive/Gutenberg/weights/weights-08-1.6394.hdf5\n",
            "Epoch 9/100\n",
            "129978/129978 [==============================] - 281s 2ms/step - loss: 1.6068\n",
            "\n",
            "Epoch 00009: loss improved from 1.63940 to 1.60682, saving model to gdrive/My Drive/Gutenberg/weights/weights-09-1.6068.hdf5\n",
            "Epoch 10/100\n",
            "129978/129978 [==============================] - 281s 2ms/step - loss: 1.5848\n",
            "\n",
            "Epoch 00010: loss improved from 1.60682 to 1.58484, saving model to gdrive/My Drive/Gutenberg/weights/weights-10-1.5848.hdf5\n",
            "Epoch 11/100\n",
            "129978/129978 [==============================] - 280s 2ms/step - loss: 1.5602\n",
            "\n",
            "Epoch 00011: loss improved from 1.58484 to 1.56016, saving model to gdrive/My Drive/Gutenberg/weights/weights-11-1.5602.hdf5\n",
            "Epoch 12/100\n",
            "129978/129978 [==============================] - 280s 2ms/step - loss: 1.5365\n",
            "\n",
            "Epoch 00012: loss improved from 1.56016 to 1.53653, saving model to gdrive/My Drive/Gutenberg/weights/weights-12-1.5365.hdf5\n",
            "Epoch 13/100\n",
            "129978/129978 [==============================] - 281s 2ms/step - loss: 1.5196\n",
            "\n",
            "Epoch 00013: loss improved from 1.53653 to 1.51959, saving model to gdrive/My Drive/Gutenberg/weights/weights-13-1.5196.hdf5\n",
            "Epoch 14/100\n",
            "129978/129978 [==============================] - 280s 2ms/step - loss: 1.5002\n",
            "\n",
            "Epoch 00014: loss improved from 1.51959 to 1.50023, saving model to gdrive/My Drive/Gutenberg/weights/weights-14-1.5002.hdf5\n",
            "Epoch 15/100\n",
            "129978/129978 [==============================] - 280s 2ms/step - loss: 1.4862\n",
            "\n",
            "Epoch 00015: loss improved from 1.50023 to 1.48619, saving model to gdrive/My Drive/Gutenberg/weights/weights-15-1.4862.hdf5\n",
            "Epoch 16/100\n",
            "129978/129978 [==============================] - 279s 2ms/step - loss: 1.4682\n",
            "\n",
            "Epoch 00016: loss improved from 1.48619 to 1.46822, saving model to gdrive/My Drive/Gutenberg/weights/weights-16-1.4682.hdf5\n",
            "Epoch 17/100\n",
            "129978/129978 [==============================] - 280s 2ms/step - loss: 1.4539\n",
            "\n",
            "Epoch 00017: loss improved from 1.46822 to 1.45394, saving model to gdrive/My Drive/Gutenberg/weights/weights-17-1.4539.hdf5\n",
            "Epoch 18/100\n",
            "129978/129978 [==============================] - 280s 2ms/step - loss: 1.4593\n",
            "\n",
            "Epoch 00018: loss did not improve from 1.45394\n",
            "Epoch 19/100\n",
            "129978/129978 [==============================] - 279s 2ms/step - loss: 1.4268\n",
            "\n",
            "Epoch 00019: loss improved from 1.45394 to 1.42676, saving model to gdrive/My Drive/Gutenberg/weights/weights-19-1.4268.hdf5\n",
            "Epoch 20/100\n",
            "129978/129978 [==============================] - 280s 2ms/step - loss: 1.4284\n",
            "\n",
            "Epoch 00020: loss did not improve from 1.42676\n",
            "Epoch 21/100\n",
            "129978/129978 [==============================] - 280s 2ms/step - loss: 1.4205\n",
            "\n",
            "Epoch 00021: loss improved from 1.42676 to 1.42049, saving model to gdrive/My Drive/Gutenberg/weights/weights-21-1.4205.hdf5\n",
            "Epoch 22/100\n",
            "129978/129978 [==============================] - 281s 2ms/step - loss: 1.3957\n",
            "\n",
            "Epoch 00022: loss improved from 1.42049 to 1.39571, saving model to gdrive/My Drive/Gutenberg/weights/weights-22-1.3957.hdf5\n",
            "Epoch 23/100\n",
            "129978/129978 [==============================] - 282s 2ms/step - loss: 1.3837\n",
            "\n",
            "Epoch 00023: loss improved from 1.39571 to 1.38371, saving model to gdrive/My Drive/Gutenberg/weights/weights-23-1.3837.hdf5\n",
            "Epoch 24/100\n",
            "129978/129978 [==============================] - 282s 2ms/step - loss: 1.3902\n",
            "\n",
            "Epoch 00024: loss did not improve from 1.38371\n",
            "Epoch 25/100\n",
            "129978/129978 [==============================] - 282s 2ms/step - loss: 1.3720\n",
            "\n",
            "Epoch 00025: loss improved from 1.38371 to 1.37196, saving model to gdrive/My Drive/Gutenberg/weights/weights-25-1.3720.hdf5\n",
            "Epoch 26/100\n",
            "129978/129978 [==============================] - 282s 2ms/step - loss: 1.3624\n",
            "\n",
            "Epoch 00026: loss improved from 1.37196 to 1.36241, saving model to gdrive/My Drive/Gutenberg/weights/weights-26-1.3624.hdf5\n",
            "Epoch 27/100\n",
            "129978/129978 [==============================] - 280s 2ms/step - loss: 1.3573\n",
            "\n",
            "Epoch 00027: loss improved from 1.36241 to 1.35729, saving model to gdrive/My Drive/Gutenberg/weights/weights-27-1.3573.hdf5\n",
            "Epoch 28/100\n",
            "129978/129978 [==============================] - 280s 2ms/step - loss: 1.3430\n",
            "\n",
            "Epoch 00028: loss improved from 1.35729 to 1.34298, saving model to gdrive/My Drive/Gutenberg/weights/weights-28-1.3430.hdf5\n",
            "Epoch 29/100\n",
            "129978/129978 [==============================] - 283s 2ms/step - loss: 1.3889\n",
            "\n",
            "Epoch 00029: loss did not improve from 1.34298\n",
            "Epoch 30/100\n",
            "129978/129978 [==============================] - 283s 2ms/step - loss: 1.3443\n",
            "\n",
            "Epoch 00030: loss did not improve from 1.34298\n",
            "Epoch 31/100\n",
            "129978/129978 [==============================] - 281s 2ms/step - loss: 1.3204\n",
            "\n",
            "Epoch 00031: loss improved from 1.34298 to 1.32042, saving model to gdrive/My Drive/Gutenberg/weights/weights-31-1.3204.hdf5\n",
            "Epoch 32/100\n",
            "129978/129978 [==============================] - 288s 2ms/step - loss: 1.3181\n",
            "\n",
            "Epoch 00032: loss improved from 1.32042 to 1.31806, saving model to gdrive/My Drive/Gutenberg/weights/weights-32-1.3181.hdf5\n",
            "Epoch 33/100\n",
            "129978/129978 [==============================] - 284s 2ms/step - loss: 1.3107\n",
            "\n",
            "Epoch 00033: loss improved from 1.31806 to 1.31067, saving model to gdrive/My Drive/Gutenberg/weights/weights-33-1.3107.hdf5\n",
            "Epoch 34/100\n",
            "129978/129978 [==============================] - 285s 2ms/step - loss: 1.3075\n",
            "\n",
            "Epoch 00034: loss improved from 1.31067 to 1.30750, saving model to gdrive/My Drive/Gutenberg/weights/weights-34-1.3075.hdf5\n",
            "Epoch 35/100\n",
            "129978/129978 [==============================] - 285s 2ms/step - loss: 1.2976\n",
            "\n",
            "Epoch 00035: loss improved from 1.30750 to 1.29758, saving model to gdrive/My Drive/Gutenberg/weights/weights-35-1.2976.hdf5\n",
            "Epoch 36/100\n",
            "129978/129978 [==============================] - 284s 2ms/step - loss: 1.2961\n",
            "\n",
            "Epoch 00036: loss improved from 1.29758 to 1.29614, saving model to gdrive/My Drive/Gutenberg/weights/weights-36-1.2961.hdf5\n",
            "Epoch 37/100\n",
            "129978/129978 [==============================] - 285s 2ms/step - loss: 1.2852\n",
            "\n",
            "Epoch 00037: loss improved from 1.29614 to 1.28524, saving model to gdrive/My Drive/Gutenberg/weights/weights-37-1.2852.hdf5\n",
            "Epoch 38/100\n",
            "129978/129978 [==============================] - 286s 2ms/step - loss: 1.2944\n",
            "\n",
            "Epoch 00038: loss did not improve from 1.28524\n",
            "Epoch 39/100\n",
            "129978/129978 [==============================] - 284s 2ms/step - loss: 1.2808\n",
            "\n",
            "Epoch 00039: loss improved from 1.28524 to 1.28078, saving model to gdrive/My Drive/Gutenberg/weights/weights-39-1.2808.hdf5\n",
            "Epoch 40/100\n",
            "129978/129978 [==============================] - 284s 2ms/step - loss: 1.2684\n",
            "\n",
            "Epoch 00040: loss improved from 1.28078 to 1.26838, saving model to gdrive/My Drive/Gutenberg/weights/weights-40-1.2684.hdf5\n",
            "Epoch 41/100\n",
            "129978/129978 [==============================] - 285s 2ms/step - loss: 1.2696\n",
            "\n",
            "Epoch 00041: loss did not improve from 1.26838\n",
            "Epoch 42/100\n",
            "129978/129978 [==============================] - 287s 2ms/step - loss: 1.2613\n",
            "\n",
            "Epoch 00042: loss improved from 1.26838 to 1.26129, saving model to gdrive/My Drive/Gutenberg/weights/weights-42-1.2613.hdf5\n",
            "Epoch 43/100\n",
            "129978/129978 [==============================] - 287s 2ms/step - loss: 1.2585\n",
            "\n",
            "Epoch 00043: loss improved from 1.26129 to 1.25849, saving model to gdrive/My Drive/Gutenberg/weights/weights-43-1.2585.hdf5\n",
            "Epoch 44/100\n",
            "129978/129978 [==============================] - 288s 2ms/step - loss: 1.2482\n",
            "\n",
            "Epoch 00044: loss improved from 1.25849 to 1.24822, saving model to gdrive/My Drive/Gutenberg/weights/weights-44-1.2482.hdf5\n",
            "Epoch 45/100\n",
            "129978/129978 [==============================] - 288s 2ms/step - loss: 1.2484\n",
            "\n",
            "Epoch 00045: loss did not improve from 1.24822\n",
            "Epoch 46/100\n",
            "129978/129978 [==============================] - 287s 2ms/step - loss: 1.2407\n",
            "\n",
            "Epoch 00046: loss improved from 1.24822 to 1.24067, saving model to gdrive/My Drive/Gutenberg/weights/weights-46-1.2407.hdf5\n",
            "Epoch 47/100\n",
            "129978/129978 [==============================] - 287s 2ms/step - loss: 1.2385\n",
            "\n",
            "Epoch 00047: loss improved from 1.24067 to 1.23851, saving model to gdrive/My Drive/Gutenberg/weights/weights-47-1.2385.hdf5\n",
            "Epoch 48/100\n",
            "129978/129978 [==============================] - 284s 2ms/step - loss: 1.2316\n",
            "\n",
            "Epoch 00048: loss improved from 1.23851 to 1.23163, saving model to gdrive/My Drive/Gutenberg/weights/weights-48-1.2316.hdf5\n",
            "Epoch 49/100\n",
            "129978/129978 [==============================] - 281s 2ms/step - loss: 1.2282\n",
            "\n",
            "Epoch 00049: loss improved from 1.23163 to 1.22816, saving model to gdrive/My Drive/Gutenberg/weights/weights-49-1.2282.hdf5\n",
            "Epoch 50/100\n",
            "129978/129978 [==============================] - 279s 2ms/step - loss: 1.2241\n",
            "\n",
            "Epoch 00050: loss improved from 1.22816 to 1.22413, saving model to gdrive/My Drive/Gutenberg/weights/weights-50-1.2241.hdf5\n",
            "Epoch 51/100\n",
            "129978/129978 [==============================] - 280s 2ms/step - loss: 1.2159\n",
            "\n",
            "Epoch 00051: loss improved from 1.22413 to 1.21593, saving model to gdrive/My Drive/Gutenberg/weights/weights-51-1.2159.hdf5\n",
            "Epoch 52/100\n",
            "129978/129978 [==============================] - 281s 2ms/step - loss: 1.2174\n",
            "\n",
            "Epoch 00052: loss did not improve from 1.21593\n",
            "Epoch 53/100\n",
            "129978/129978 [==============================] - 281s 2ms/step - loss: 1.2098\n",
            "\n",
            "Epoch 00053: loss improved from 1.21593 to 1.20976, saving model to gdrive/My Drive/Gutenberg/weights/weights-53-1.2098.hdf5\n",
            "Epoch 54/100\n",
            "129978/129978 [==============================] - 284s 2ms/step - loss: 1.2094\n",
            "\n",
            "Epoch 00054: loss improved from 1.20976 to 1.20936, saving model to gdrive/My Drive/Gutenberg/weights/weights-54-1.2094.hdf5\n",
            "Epoch 55/100\n",
            "129978/129978 [==============================] - 279s 2ms/step - loss: 1.2041\n",
            "\n",
            "Epoch 00055: loss improved from 1.20936 to 1.20407, saving model to gdrive/My Drive/Gutenberg/weights/weights-55-1.2041.hdf5\n",
            "Epoch 56/100\n",
            "129978/129978 [==============================] - 285s 2ms/step - loss: 1.1952\n",
            "\n",
            "Epoch 00056: loss improved from 1.20407 to 1.19520, saving model to gdrive/My Drive/Gutenberg/weights/weights-56-1.1952.hdf5\n",
            "Epoch 57/100\n",
            "129978/129978 [==============================] - 281s 2ms/step - loss: 1.2008\n",
            "\n",
            "Epoch 00057: loss did not improve from 1.19520\n",
            "Epoch 58/100\n",
            "129978/129978 [==============================] - 283s 2ms/step - loss: 1.1936\n",
            "\n",
            "Epoch 00058: loss improved from 1.19520 to 1.19358, saving model to gdrive/My Drive/Gutenberg/weights/weights-58-1.1936.hdf5\n",
            "Epoch 59/100\n",
            "129978/129978 [==============================] - 282s 2ms/step - loss: 1.1898\n",
            "\n",
            "Epoch 00059: loss improved from 1.19358 to 1.18983, saving model to gdrive/My Drive/Gutenberg/weights/weights-59-1.1898.hdf5\n",
            "Epoch 60/100\n",
            "129978/129978 [==============================] - 281s 2ms/step - loss: 1.1843\n",
            "\n",
            "Epoch 00060: loss improved from 1.18983 to 1.18435, saving model to gdrive/My Drive/Gutenberg/weights/weights-60-1.1843.hdf5\n",
            "Epoch 61/100\n",
            " 21376/129978 [===>..........................] - ETA: 3:56 - loss: 1.1659Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC3hbptX-ZtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vquF5nsxAtPD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "outputId": "9ac25ae7-3abb-4bfa-e82a-2c9f57df1fcb"
      },
      "source": [
        "model.fit(X, label, epochs=100, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0727 05:59:03.973306 139949467694976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0727 05:59:03.980644 139949467694976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0727 05:59:04.002296 139949467694976 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0727 05:59:04.028950 139949467694976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0727 05:59:04.785511 139949467694976 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0727 05:59:04.965510 139949467694976 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            " 10240/129978 [=>............................] - ETA: 5:27 - loss: 2.8438"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-ff0f883a2edc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m-hzSth-Tny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('weights-99-1.1009.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93JpEsToE2s5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(seed_text, next_words, max_sequence_len):\n",
        "  for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    test = np.expand_dims(token_list, axis=-1)\n",
        "    predicted = model.predict_classes(test, verbose=0)\n",
        "    \n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == predicted:\n",
        "        output_word = word\n",
        "        break\n",
        "    seed_text += output_word\n",
        "  return seed_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3uaepIcE39Y",
        "colab_type": "code",
        "outputId": "46763382-c390-46fa-bc2a-2ae59a4f01ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(generate_text(\"be no mistake about it: it was neither more nor less than a pig, and she felt that it would be quit be no mistake about it: it was neither more nor less than a pig, and she felt that it would be quit\", 500, max_sequence_len))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "be no mistake about it: it was neither more nor less than a pig, and she felt that it would be quit be no mistake about it: it was neither more nor less than a pig, and she felt that it would be quite fore for som ond then the dabbit asger eat it wost thiughts the cryphon raid the kiot as iffended toble and alice said tolted and then the dabbits asgrel toble toble and then the dabbits asgrel toble toble and then the dabbits asgrel toble toble and then the dabbits asgrel toble toble and then the dabbits asgrel toble toble and then the dabbits asgrel toble toble and then the dabbits asgrel toble toble and then the dabbits asgrel toble toble and then the dabbits asgrel toble toble and then the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD_Jg2cSDdbN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "2b35467d-2ddf-4884-9825-38185bcd5959"
      },
      "source": [
        "print(generate_text(\"Cat and \", 500, max_sequence_len))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cat and wioked at the mat reemed tolts op sithin wet me said alice in thme toge so monduyt soid alicd in a mriwu of il and sow mome toltes and sow sot sot sot sot sot sot sot alice said to htself in the ofcurred ofaing toble and then the dabbits asgrel toble toble and then the dabbits asgrel toble toble and then the dabbits asgrel toble toble and then the dabbits asgrel toble toble and then the dabbits asgrel toble toble and then the dabbits asgrel toble toble and then the dabbits asgrel toble toble and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3p61mrUD2Pp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "1f12afc8-10e1-4394-ad49-72f38923f56b"
      },
      "source": [
        "print(generate_text(\"So she was considering in her own mind\", 500, max_sequence_len))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "So she was considering in her own mindidt if ilea was sit a cor sot a mot ir the satter ar she said to htself ht sow h save dont then the dobbit seatly depained lownt in wing asdougc bor awices wive a lig ond then the dabbit asger eat it wost thiughts the cryphon raid the kiot as iffended toble and alice said tolted and then the dabbits asgrel toble toble and then the dabbits asgrel toble toble and then the dabbits asgrel toble toble and then the dabbits asgrel toble toble and then the dabbits asgrel toble toble and then the dabbits\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}